import pandas as pd
import numpy as np
import time
import sys
import csv
import re ##regular expressions
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.model_selection import train_test_split
#from sklearn.feature_extraction.text import CountVectorizer
#from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords

###Set up the classifier 

cores = 3 # how many cpu cores one wants to use (for most classifiers)

randforclf = False ##use the random forest classifier in this run?
no_of_trees = 240 #number of trees in the forest of the random forest classifier

adaboost = True #use adaboost in this run?
no_est_ada = 140 #number of weak estimators used for adaboost


### Load & Filter dataset
print("Load Dataset & Filter")
df = pd.read_csv("../input/hate-speech-and-offensive-language-dataset/labeled_data.csv")
df_selected = df.drop(['Unnamed: 0','count', "offensive_language", "neither", 'class'], axis=1)
df_selected = df_selected.fillna(' ')
df_selected["tweet"]df_selected.apply(lambda x: " ".join([for i in re.sub("[^a-zA-Z]"," ",x).split() if i not in stop]).lower())

### convert remaining data to machine learning readable form
print("Convert to Array")
hate_speech = np.asarray(df_selected.hate_speech)
tweets = np.asarray(df_selected.tweet)
train_len = len(tweets)





### import prediction data

### Load & Filter dataset
print("Load Dataset & Filter")

###eurosoc
df_soc = pd.read_csv('../input/eurosocsong/eurosoc.csv',
                 lineterminator='\n')
df_soc_selected = df_soc.drop(['Datetime','Username'], axis=1)

###eurosong
df_song = pd.read_csv('../input/eurosocsong/eurosong.csv',
                 lineterminator='\n')
df_song_selected = df_song.drop(['Datetime','Username'], axis=1)


### convert remaining data to machine learning readable form
print("Convert to Array")
text_soc = np.asarray(df_soc_selected.Text)

text_song = np.asarray(df_song_selected.Text)

###combine training data and prediction data
combined = np.concatenate((tweets, text_soc))
combined_len = len(combined)




###split training data and prediction data
train = tweets_tfidf[0:train_len]
soc = tweets_tfidf[train_len:]

###check if split was succesful
print("Train_len: ", train_len)
print("Train: ", train.shape)
print("Soc_len: ", combined_len - train_len)
print("Soc: ", soc.shape)


### train the classifier using the training data


if randforclf == True:

    print(f"Training Random Forest Classifier with {no_of_trees} estimators")
    start = time.time()
    clf = RandomForestClassifier(n_estimators=no_of_trees, n_jobs=cores)
    clf.fit(train, hate_speech)

    end = time.time()
    print ("Training Time: ", end - start)
    


if adaboost == True:
    start = time.time()
    print(f"training AdaBoost with {no_est_ada} estimators")
    clf = AdaBoostClassifier(n_estimators=no_est_ada)  # , n_jobs=cores)
    clf.fit(train, hate_speech)

    end = time.time()
    print ("Training Time: ", end - start)
    


### Prediction
start2 = time.time()    

hate_speech_new_soc = clf.predict(soc)
countcount = 0
for item in hate_speech_new_soc:
    if item == 1:
        #print("1")
        countcount+=1
print("Number of hatespeech-tweets detected: "+ str(countcount))
print("Prediction_len: ", len(hate_speech_new_soc))

end2 = time.time()
print ("Prediction Time: ", end2 - start2)

### Save Prediction to csv
countcount = 0
l1 = list(zip(hate_speech_new_soc, text_soc))
for hate in l1:
    if hate[0] == 1:
        print(hate[1])
        countcount +=1
print(countcount)


with open('my.csv','wt') as out:
   csv_out=csv.writer(out)
   csv_out.writerows(l1)
